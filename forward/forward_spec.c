// Sketch of IR code needed to produce what is currently generated by the
// Trellis compiler.

type HMM {
  f32 gamma;
  f32 *trans1;
  f32 *trans2;
  f32 *output_prob;
  f32 *initial_prob;
}

type ObsSeqs {
  u8 *data;
  i64 *lens;
  i64 maxlen;
  i64 num_instances;
}

// We could support external function calls, but there are a few caveats that
// the user needs to consider.
//
// First, users should avoid using, e.g., threadIdx or blockIdx directly in an
// external function used from the IR, because the IR compiler will decide how
// to map tasks to the GPU. The preferred approach should be to pass the task
// index to the external function instead. However, a user should be able to
// annotate a task (probably where the task is launched) to control how it is
// mapped to GPU threads, and based on that, safely make assumptions in
// external code.
//
// Second, external functions called from a task have to be declared with the
// __device__ attribute (in CUDA). Similarly, an external function called from
// a "regular" function cannot be declared only with __device__ (in CUDA), as
// this makes it unavailable from host (CPU) code. Perhaps similar annotations
// should be required in the IR?
extern i64 forward_prob_predecessors(f32 *alpha_prev, i64 instance, u16 state, f32 *probs);

task forward_init(HMM hmm, ObsSeqs seqs, f32 *alpha_src) {
  i64 instance = task_index;
  u8 o = seqs.data[instance * seqs.maxlen];
  parallel for state in 0 to 1024 {
    alpha_src[instance * 1024 + state] =
      hmm.initial_prob[state] + hmm.output_prob[o * 64 + state % 64];
  }
}

task forward_step(HMM hmm, ObsSeqs seqs, f32 *alpha_src, f32 *alpha_dst, i64 t, f32 neginf) {
  i64 instance = task_index;
  parallel for state in 0 to 1024 {
    i64 idx = instance * 1024 + state;
    if (t < seqs.lens[instance]) {
      u8 o = seqs.data[instance * seqs.maxlen + t];
      f32 probs[5];
      i64 pidx = forward_prob_predecessors(alpha_prev, instance, state, probs);
      while (pidx < 5) {
        probs[pidx] = neginf;
        pidx = pidx + 1;
      }
      alpha_curr[idx] = lse(probs, neginf) + hmm.output_prob[o * 64 + state % 64];
    } else if (t == seqs.lens[instance]) {
      alpha_curr[idx] = alpha_prev[idx];
    }
  }
}

task forward_lse(HMM hmm, f32 *alpha_last, f32 neginf, f32 *result) {
  i64 instance = task_index;

  // Compute the maximum value
  f32 maxp = neginf;
  parallel for state in 0 to 1024 {
    maxp = max_f32(alpha_last[state], maxp);
  }

  // Compute the sum of the exponentiated values subtracted by the maximum (for
  // numerical precision).
  f32 sum = 0.0;
  parallel for state in 0 to 1024 {
    sum = add_f32(sum, exp_f32(sub_f32(alpha_last[state], maxp)));
  }

  // Write the resulting sum to global memory.
  result[instance] = sum;
}

fn forward(HMM hmm, ObsSeqs seqs, f32 *result) {
  // Allocate temporary data on the target (GPU). Alternatively, this could all
  // be offloaded by having the caller pass these buffers as arguments.
  f32 alpha_src[100 * 1024];
  f32 alpha_dst[100 * 1024];

  // Initialization step
  launch forward_init[100](hmm, seqs, alpha_src);

  // Forward step
  f32 neginf = -1.0 / 0.0;
  for t in 1 to maxlen {
    launch forward_step[100](hmm, seqs, alpha_src, alpha_dst, t, neginf);

    // Swap pointers
    f32 *tmp = alpha_src;
    alpha_src = alpha_dst;
    alpha_dst = tmp;
  }

  // LogSumExp step
  launch forward_lse[100](hmm, alpha_src, neginf, result);
}
